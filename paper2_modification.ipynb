{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012349c5-8978-48d7-ac45-cacc85668d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.0163\n",
      "Epoch [2/30], Loss: 0.0079\n",
      "Epoch [3/30], Loss: 0.0055\n",
      "Epoch [4/30], Loss: 0.0041\n",
      "Epoch [5/30], Loss: 0.0038\n",
      "Epoch [6/30], Loss: 0.0029\n",
      "Epoch [7/30], Loss: 0.0025\n",
      "Epoch [8/30], Loss: 0.0022\n",
      "Epoch [9/30], Loss: 0.0018\n",
      "Epoch [10/30], Loss: 0.0018\n",
      "Epoch [11/30], Loss: 0.0013\n",
      "Epoch [12/30], Loss: 0.0012\n",
      "Epoch [13/30], Loss: 0.0011\n",
      "Epoch [14/30], Loss: 0.0011\n",
      "Epoch [15/30], Loss: 0.0012\n",
      "Epoch [16/30], Loss: 0.0009\n",
      "Epoch [17/30], Loss: 0.0009\n",
      "Epoch [18/30], Loss: 0.0010\n",
      "Epoch [19/30], Loss: 0.0008\n",
      "Epoch [20/30], Loss: 0.0007\n",
      "Epoch [21/30], Loss: 0.0009\n",
      "Epoch [22/30], Loss: 0.0007\n",
      "Epoch [23/30], Loss: 0.0006\n",
      "Epoch [24/30], Loss: 0.0006\n",
      "Epoch [25/30], Loss: 0.0006\n",
      "Epoch [26/30], Loss: 0.0007\n",
      "Epoch [27/30], Loss: 0.0006\n",
      "Epoch [28/30], Loss: 0.0005\n",
      "Epoch [29/30], Loss: 0.0005\n",
      "Epoch [30/30], Loss: 0.0005\n",
      "Accuracy: 0.7732\n",
      "Confusion Matrix:\n",
      " [[ 14   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0]\n",
      " [  0 188   1  12   2   0   0   0   0  13  13  40   0   0   0   0]\n",
      " [  0   0 106  11   0   0   0   0   0   3  40  16   1   0   0   0]\n",
      " [  0   0   0  37   0   0   0   0   0   0   0   1   0   0   1   0]\n",
      " [  0   0   0   0  96   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   2 153   0   0   0   0   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   4   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 105   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0  12   0   0   1   0   0   0   0 186   0   9   0   0   0   0]\n",
      " [  0  47   1   0   2   0   0   0   0 121 227  71   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0 121   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  34   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0   0   0   0   1 209  27   0]\n",
      " [  0   0   0   0   0   8   0   0   0   0   0   0   0   2  87   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8750    0.9333        16\n",
      "           1     0.7611    0.6989    0.7287       269\n",
      "           2     0.9725    0.5989    0.7413       177\n",
      "           3     0.6167    0.9487    0.7475        39\n",
      "           4     0.9057    0.9897    0.9458        97\n",
      "           5     0.9503    0.9808    0.9653       156\n",
      "           6     0.8000    1.0000    0.8889         4\n",
      "           7     1.0000    1.0000    1.0000       105\n",
      "           8     1.0000    1.0000    1.0000         2\n",
      "           9     0.5759    0.8942    0.7006       208\n",
      "          10     0.8107    0.4840    0.6061       469\n",
      "          11     0.4672    0.9918    0.6352       122\n",
      "          12     0.9444    1.0000    0.9714        34\n",
      "          13     0.9905    0.8745    0.9289       239\n",
      "          14     0.7500    0.8969    0.8169        97\n",
      "          15     1.0000    1.0000    1.0000        16\n",
      "\n",
      "    accuracy                         0.7732      2050\n",
      "   macro avg     0.8466    0.8896    0.8506      2050\n",
      "weighted avg     0.8184    0.7732    0.7717      2050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "# Load the hyperspectral data and ground-truth labels\n",
    "data = sio.loadmat('Indian_pines_corrected.mat')  # Replace with your file path\n",
    "ground_truth_data = sio.loadmat('Indian_pines_gt.mat')  # Replace with your file path\n",
    "\n",
    "# Extract hyperspectral image and ground truth\n",
    "hsi_data = data['indian_pines_corrected']  # HSI data, shape (145, 145, 200)\n",
    "ground_truth = ground_truth_data['indian_pines_gt']  # Ground truth labels, shape (145, 145)\n",
    "\n",
    "def extract_patches(hsi_data, ground_truth, patch_size=3):\n",
    "    pad_size = patch_size // 2\n",
    "    hsi_padded = np.pad(hsi_data, ((pad_size, pad_size), (pad_size, pad_size), (0, 0)), mode='reflect')\n",
    "    \n",
    "    patches = []\n",
    "    labels = []\n",
    "    for i in range(pad_size, hsi_data.shape[0] + pad_size):\n",
    "        for j in range(pad_size, hsi_data.shape[1] + pad_size):\n",
    "            patch = hsi_padded[i - pad_size:i + pad_size + 1, j - pad_size:j + pad_size + 1, :]\n",
    "            patches.append(patch)\n",
    "            labels.append(ground_truth[i - pad_size, j - pad_size])\n",
    "    \n",
    "    patches = np.array(patches)\n",
    "    labels = np.array(labels)\n",
    "    valid_indices = labels > 0\n",
    "    return patches[valid_indices], labels[valid_indices] - 1  # Only keep labeled pixels\n",
    "\n",
    "# Example usage with patch extraction\n",
    "patch_size = 3\n",
    "data_patches, labels = extract_patches(hsi_data, ground_truth, patch_size=patch_size)\n",
    "data_patches = torch.tensor(data_patches, dtype=torch.float32).permute(0, 3, 1, 2)  # [N, bands, patch_size, patch_size]\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data_patches, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_train = data_train.clone().detach()\n",
    "data_test = data_test.clone().detach()\n",
    "labels_train = labels_train.clone().detach()\n",
    "labels_test = labels_test.clone().detach()\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(data_train, labels_train)\n",
    "test_dataset = TensorDataset(data_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Spatial Attention Block\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.reduce_conv = nn.Conv2d(2, 1, kernel_size=1)  # 1x1 conv to reduce to 1 channel\n",
    "        self.conv = nn.Conv2d(1, 1, kernel_size=7, padding=3, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        \n",
    "        out = F.relu(self.reduce_conv(out))  # Reduce to 1 channel before spatial attention\n",
    "        out = torch.sigmoid(self.conv(out))\n",
    "        return x * out\n",
    "\n",
    "# Model with Spatial-Spectral Attention \n",
    "class ImprovedHyperspectralCNN(nn.Module):\n",
    "    def __init__(self, num_bands, num_classes):\n",
    "        super(ImprovedHyperspectralCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_bands, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.spatial_att1 = SpatialAttention(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.spectral_att1 = SpatialAttention(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.spectral_att2 = SpatialAttention(256)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.spatial_att1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.spectral_att1(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.spectral_att2(x)\n",
    "        \n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Focal Loss with Class Weights\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute standard Cross Entropy Loss with no reduction\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "\n",
    "        # Gather the correct class weight for each target in the batch\n",
    "        alpha_t = self.alpha[targets] if self.alpha is not None else 1.0\n",
    "\n",
    "        # Compute Focal Loss with class weights applied per sample\n",
    "        F_loss = (alpha_t * (1 - pt) ** self.gamma * BCE_loss).mean()\n",
    "        return F_loss\n",
    "\n",
    "# Compute class weights and initialize Focal Loss\n",
    "class_counts = torch.bincount(labels_train)\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "focal_loss = FocalLoss(alpha=class_weights.to(data_train.device))\n",
    "\n",
    "# Training parameters\n",
    "num_classes = len(torch.unique(labels_train))\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 30\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = ImprovedHyperspectralCNN(num_bands=data_patches.shape[1], num_classes=num_classes)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        outputs = model(data)\n",
    "        loss = focal_loss(outputs, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, digits=4)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy, test_conf_matrix, test_class_report = evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c64cd72-a000-445d-89c5-cf3f872600d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Function to get attention scores from the spectral attention layers\n",
    "def get_combined_spectral_attention_scores(model, data_loader, num_bands=200):\n",
    "    spectral_attention_scores_layer1 = []\n",
    "    spectral_attention_scores_layer2 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            # Forward pass through each layer to capture spectral attention scores\n",
    "            x = F.relu(model.bn1(model.conv1(data)))\n",
    "            x = model.spatial_att1(x)\n",
    "            \n",
    "            x = F.relu(model.bn2(model.conv2(x)))\n",
    "            a1 = model.spectral_att1(x)  # First spectral attention layer\n",
    "            spectral_attention_scores_layer1.append(a1.cpu().numpy().mean(axis=(2, 3)))  # Average across spatial dimensions\n",
    "\n",
    "            x = F.relu(model.bn3(model.conv3(x)))\n",
    "            a2 = model.spectral_att2(x)  # Second spectral attention layer\n",
    "            spectral_attention_scores_layer2.append(a2.cpu().numpy().mean(axis=(2, 3)))  # Average across spatial dimensions\n",
    "\n",
    "    # Concatenate scores across all batches for each layer and compute the mean\n",
    "    spectral_attention_scores_layer1 = np.concatenate(spectral_attention_scores_layer1, axis=0).mean(axis=0)\n",
    "    spectral_attention_scores_layer2 = np.concatenate(spectral_attention_scores_layer2, axis=0).mean(axis=0)\n",
    "    \n",
    "    # Normalize scores to the number of original bands\n",
    "    combined_scores = np.concatenate([spectral_attention_scores_layer1, spectral_attention_scores_layer2])\n",
    "    \n",
    "    # Reshape or truncate to match the number of original spectral bands\n",
    "    if combined_scores.shape[0] > num_bands:\n",
    "        combined_scores = combined_scores[:num_bands]\n",
    "    elif combined_scores.shape[0] < num_bands:\n",
    "        combined_scores = np.pad(combined_scores, (0, num_bands - combined_scores.shape[0]), mode='constant')\n",
    "\n",
    "    return combined_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d4f6f83-97e3-4c72-b4fd-d14ce9954dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Important Bands Selected: [139, 146, 188, 192]\n"
     ]
    }
   ],
   "source": [
    "# Extract combined mean attention scores aligned to original band count\n",
    "mean_attention_scores = get_combined_spectral_attention_scores(model, train_loader, num_bands=data_patches.shape[1])\n",
    "\n",
    "# Function to select important bands using EllipticEnvelope\n",
    "def select_important_bands(attention_scores, contamination_rate):\n",
    "    \"\"\"\n",
    "    Selects important bands using EllipticEnvelope with an adjusted support_fraction.\n",
    "    \"\"\"\n",
    "    env_model = EllipticEnvelope(contamination=contamination_rate, support_fraction=0.5)\n",
    "    selected_bands = env_model.fit_predict(attention_scores.reshape(-1, 1))\n",
    "    important_band_indices = [i for i, v in enumerate(selected_bands) if v == -1]\n",
    "    return important_band_indices\n",
    "\n",
    "# Identify important bands based on combined attention scores\n",
    "important_bands = select_important_bands(mean_attention_scores, contamination_rate=0.02)\n",
    "print(\"Most Important Bands Selected:\", important_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b9458-e4e0-407b-ae48-8d512ee1a7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
